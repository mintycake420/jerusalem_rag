{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jerusalem RAG Explorer - Complete Demo\n",
        "\n",
        "## A Retrieval-Augmented Generation System for Crusader History Research\n",
        "\n",
        "**By Yotam Nachtomy-Katz**  \n",
        "**ID: 211718366**  \n",
        "**Submitted: 01.02.26**  \n",
        "**Course: Information Retrieval**  \n",
        "**Haifa University**\n",
        "\n",
        "---\n",
        "\n",
        "This notebook provides a complete demonstration of the Jerusalem RAG Explorer system, from data ingestion to question answering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Project Overview](#1-project-overview)\n",
        "2. [System Architecture](#2-system-architecture)\n",
        "3. [Setup](#3-setup)\n",
        "4. [Data Pipeline Demo](#4-data-pipeline-demo)\n",
        "5. [Retrieval Demo](#5-retrieval-demo)\n",
        "6. [Question Answering Demo](#6-question-answering-demo)\n",
        "7. [Response Modes](#7-response-modes)\n",
        "8. [Conclusion](#8-conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Project Overview\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Researching the Crusades presents unique challenges:\n",
        "- **Language Barriers**: Primary sources exist in Latin, Arabic, Greek, Armenian, and Old French\n",
        "- **Scattered Archives**: Documents are distributed across multiple digital repositories\n",
        "- **Volume**: Thousands of pages must be manually searched to find relevant passages\n",
        "- **Perspective Bias**: Western sources dominate; Eastern perspectives are underrepresented\n",
        "\n",
        "### Solution\n",
        "\n",
        "Jerusalem RAG Explorer addresses these challenges through:\n",
        "- **Multilingual Corpus**: Aggregates Latin, Arabic, Greek, Armenian, and French sources\n",
        "- **AI Translation**: Pre-translates non-English texts during ingestion\n",
        "- **Semantic Search**: FAISS index enables natural language queries\n",
        "- **Grounded Answers**: LLM generates responses with mandatory source citations\n",
        "- **Comparative Analysis**: Compare Western, Eastern, and Byzantine perspectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. System Architecture\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                        DATA INGESTION                           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ  Archive.org  ‚îÄ‚îÄ‚îê                                               ‚îÇ\n",
        "‚îÇ  Gallica (BnF) ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ Fetch ‚îÄ‚îÄ‚ñ∂ Chunk ‚îÄ‚îÄ‚ñ∂ Translate ‚îÄ‚îÄ‚ñ∂ Embed   ‚îÇ\n",
        "‚îÇ  Wikipedia ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
        "‚îÇ                                            ‚îÇ                    ‚îÇ\n",
        "‚îÇ                                            ‚ñº                    ‚îÇ\n",
        "‚îÇ                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ\n",
        "‚îÇ                                    ‚îÇ FAISS Index  ‚îÇ             ‚îÇ\n",
        "‚îÇ                                    ‚îÇ + Metadata   ‚îÇ             ‚îÇ\n",
        "‚îÇ                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                        QUERY PIPELINE                           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îÇ  User Question ‚îÄ‚îÄ‚ñ∂ Embed ‚îÄ‚îÄ‚ñ∂ FAISS Search ‚îÄ‚îÄ‚ñ∂ Top-K Chunks     ‚îÇ\n",
        "‚îÇ                                                      ‚îÇ          ‚îÇ\n",
        "‚îÇ                                                      ‚ñº          ‚îÇ\n",
        "‚îÇ                              Context + Prompt ‚îÄ‚îÄ‚ñ∂ Gemini LLM    ‚îÇ\n",
        "‚îÇ                                                      ‚îÇ          ‚îÇ\n",
        "‚îÇ                                                      ‚ñº          ‚îÇ\n",
        "‚îÇ                                          Answer with Citations  ‚îÇ\n",
        "‚îÇ                                                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Technology Stack\n",
        "\n",
        "| Component | Technology | Purpose |\n",
        "|-----------|------------|---------|  \n",
        "| Frontend | Streamlit | Interactive web UI |\n",
        "| Embeddings | sentence-transformers | 384-dim text vectors |\n",
        "| Vector Search | FAISS | Fast similarity search |\n",
        "| LLM | Google Gemini | Answer generation |\n",
        "| Translation | Gemini API | Medieval text translation |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "# Add parent directory to path for imports\n",
        "sys.path.insert(0, str(Path(\"..\").absolute()))\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n",
        "print(f\"GEMINI_API_KEY: {'‚úì Found' if os.getenv('GEMINI_API_KEY') else '‚úó Missing'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "INDEX_DIR = Path(\"../data/index_v2\")\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Language utilities\n",
        "LANGUAGE_NAMES = {\"en\": \"English\", \"la\": \"Latin\", \"ar\": \"Arabic\", \"el\": \"Greek\", \"fr\": \"French\", \"hy\": \"Armenian\"}\n",
        "LANGUAGE_FLAGS = {\"en\": \"üá¨üáß\", \"la\": \"üáªüá¶\", \"ar\": \"üá∏üá¶\", \"el\": \"üá¨üá∑\", \"fr\": \"üá´üá∑\", \"hy\": \"üá¶üá≤\"}\n",
        "\n",
        "def get_lang_name(code): return LANGUAGE_NAMES.get(code, code.upper())\n",
        "def get_lang_flag(code): return LANGUAGE_FLAGS.get(code, \"üåê\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Pipeline Demo\n",
        "\n",
        "### 4.1 Document Sources\n",
        "\n",
        "The system fetches documents from:\n",
        "- **Archive.org**: Recueil des historiens des croisades (Latin, Arabic, Greek, Armenian)\n",
        "- **Gallica (BnF)**: French National Library manuscripts\n",
        "- **Wikipedia**: Modern encyclopedic content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample source documents\n",
        "data_dir = Path(\"../data/raw\")\n",
        "\n",
        "if data_dir.exists():\n",
        "    txt_files = list(data_dir.rglob(\"*.txt\"))\n",
        "    print(f\"Total documents in corpus: {len(txt_files)}\")\n",
        "    print(\"\\nSample documents:\")\n",
        "    for f in txt_files[:5]:\n",
        "        print(f\"  - {f.name}\")\n",
        "else:\n",
        "    print(\"Data directory not found. Run 01_data_fetching.ipynb first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Chunking Strategy\n",
        "\n",
        "Documents are split into overlapping segments:\n",
        "- **Chunk size**: 2000 characters\n",
        "- **Overlap**: 300 characters (preserves context across boundaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=2000, overlap=300):\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i + chunk_size].strip())\n",
        "        i += chunk_size - overlap\n",
        "    return [c for c in chunks if c]\n",
        "\n",
        "# Demonstrate\n",
        "demo_text = \"A\" * 5000\n",
        "demo_chunks = chunk_text(demo_text)\n",
        "print(f\"Demo: {len(demo_text)} chars ‚Üí {len(demo_chunks)} chunks\")\n",
        "print(f\"Chunk sizes: {[len(c) for c in demo_chunks]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Load Processed Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "print(f\"Loading embedding model: {MODEL_NAME}\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "# Load FAISS index\n",
        "print(f\"Loading FAISS index...\")\n",
        "index = faiss.read_index(str(INDEX_DIR / \"faiss.index\"))\n",
        "print(f\"  Index contains {index.ntotal} vectors\")\n",
        "\n",
        "# Load chunks\n",
        "print(f\"Loading chunks metadata...\")\n",
        "with open(INDEX_DIR / \"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "print(f\"  Loaded {len(chunks)} chunks\")\n",
        "\n",
        "# Count by language\n",
        "lang_counts = {}\n",
        "for c in chunks:\n",
        "    lang = c.get(\"language\", \"en\")\n",
        "    lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
        "\n",
        "print(\"\\nChunks by language:\")\n",
        "for lang, count in sorted(lang_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {get_lang_flag(lang)} {get_lang_name(lang)}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Retrieval Demo\n",
        "\n",
        "The retrieval system:\n",
        "1. Embeds the query using the same model\n",
        "2. Searches the FAISS index for similar vectors\n",
        "3. Returns top-k chunks with relevance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve(question, top_k=6, languages=None):\n",
        "    \"\"\"Retrieve top-k relevant chunks.\"\"\"\n",
        "    # Embed query\n",
        "    q_emb = model.encode([question], normalize_embeddings=True)\n",
        "    q_emb = np.array(q_emb, dtype=\"float32\")\n",
        "    \n",
        "    # Search\n",
        "    search_k = top_k * 3 if languages else top_k\n",
        "    scores, ids = index.search(q_emb, search_k)\n",
        "    \n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], ids[0]):\n",
        "        if idx < 0 or idx >= len(chunks):\n",
        "            continue\n",
        "        chunk = chunks[idx]\n",
        "        \n",
        "        # Language filter\n",
        "        if languages:\n",
        "            if chunk.get(\"language\", \"en\") not in languages:\n",
        "                continue\n",
        "        \n",
        "        results.append((float(score), chunk))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo retrieval\n",
        "question = \"What happened at the Battle of Hattin?\"\n",
        "results = retrieve(question, top_k=5)\n",
        "\n",
        "print(f\"Query: '{question}'\\n\")\n",
        "print(f\"Retrieved {len(results)} chunks:\\n\")\n",
        "\n",
        "for i, (score, chunk) in enumerate(results):\n",
        "    lang = chunk.get(\"language\", \"en\")\n",
        "    flag = get_lang_flag(lang)\n",
        "    is_trans = \"(translated)\" if chunk.get(\"is_translation\") else \"\"\n",
        "    \n",
        "    print(f\"{i+1}. [{chunk['chunk_id']}] Score: {score:.3f} {flag} {is_trans}\")\n",
        "    print(f\"   Preview: {chunk['text'][:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Question Answering Demo\n",
        "\n",
        "The complete RAG pipeline:\n",
        "1. **Retrieve** relevant chunks\n",
        "2. **Format** context with metadata\n",
        "3. **Generate** answer with Gemini LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a scholarly historian specializing in the Crusades (1095-1291 CE).\n",
        "\n",
        "RULES:\n",
        "1. Answer ONLY using the provided CONTEXT\n",
        "2. EVERY claim must cite [ChunkID]\n",
        "3. Note original language of translated sources\n",
        "4. If insufficient information, say so\n",
        "\"\"\"\n",
        "\n",
        "def format_context(results):\n",
        "    \"\"\"Format chunks for LLM.\"\"\"\n",
        "    parts = []\n",
        "    for score, chunk in results:\n",
        "        header = f\"[{chunk['chunk_id']}] (score: {score:.3f})\"\n",
        "        if chunk.get(\"original_language\"):\n",
        "            header += f\" [Translated from {get_lang_name(chunk['original_language'])}]\"\n",
        "        parts.append(f\"{header}\\n{chunk['text']}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)\n",
        "\n",
        "def ask_question(question, mode=\"default\", top_k=6):\n",
        "    \"\"\"Complete RAG pipeline.\"\"\"\n",
        "    # Retrieve\n",
        "    results = retrieve(question, top_k=top_k)\n",
        "    if not results:\n",
        "        return \"No relevant sources found.\", []\n",
        "    \n",
        "    # Format context\n",
        "    context = format_context(results)\n",
        "    \n",
        "    # Build prompt\n",
        "    prompt = f\"{SYSTEM_PROMPT}\\n\\nQUESTION: {question}\\n\\nCONTEXT:\\n{context}\\n\\nANSWER:\"\n",
        "    \n",
        "    # Generate\n",
        "    client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    config = types.GenerateContentConfig(temperature=0.3, max_output_tokens=4096)\n",
        "    resp = client.models.generate_content(\n",
        "        model=\"gemini-3-flash-preview\",\n",
        "        contents=prompt,\n",
        "        config=config\n",
        "    )\n",
        "    \n",
        "    return resp.text or \"\", results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo question answering\n",
        "question = \"What happened at the Battle of Hattin?\"\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "answer, sources = ask_question(question, top_k=6)\n",
        "\n",
        "print(\"\\nANSWER:\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SOURCES:\")\n",
        "for score, chunk in sources:\n",
        "    flag = get_lang_flag(chunk.get(\"language\", \"en\"))\n",
        "    print(f\"  {flag} {chunk['chunk_id']} (score: {score:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Response Modes\n",
        "\n",
        "The system supports multiple response formats:\n",
        "\n",
        "| Mode | Description |\n",
        "|------|-------------|\n",
        "| **default** | Scholarly prose with citations |\n",
        "| **chronology** | Timeline format |\n",
        "| **dossier** | Structured report |\n",
        "| **comparative** | Cross-cultural analysis |\n",
        "| **claim_check** | Fact verification |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different questions\n",
        "questions = [\n",
        "    \"Who was Baldwin IV of Jerusalem?\",\n",
        "    \"What were the laws of the Kingdom of Jerusalem?\",\n",
        "    \"How did Arabic sources describe the Crusaders?\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Q: {q}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    answer, sources = ask_question(q, top_k=4)\n",
        "    print(f\"\\nA: {answer[:500]}...\" if len(answer) > 500 else f\"\\nA: {answer}\")\n",
        "    print(f\"\\n[{len(sources)} sources used]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "### Summary\n",
        "\n",
        "Jerusalem RAG Explorer demonstrates how RAG systems can:\n",
        "- **Bridge language barriers** through AI translation\n",
        "- **Enable semantic search** across historical documents\n",
        "- **Maintain scholarly rigor** through mandatory citations\n",
        "- **Reveal multiple perspectives** on historical events\n",
        "\n",
        "### Key Achievements\n",
        "\n",
        "1. **Multilingual Corpus**: Latin, Arabic, Greek, Armenian, French sources\n",
        "2. **Pre-translation Pipeline**: Non-English texts translated during ingestion\n",
        "3. **Semantic Retrieval**: FAISS index with 384-dim embeddings\n",
        "4. **Grounded Generation**: Every answer cites specific sources\n",
        "5. **Comparative Analysis**: Compare Eastern vs Western perspectives\n",
        "\n",
        "### Future Work\n",
        "\n",
        "- Fine-tune embeddings on medieval historical text\n",
        "- Add more sources (Vatican Library, British Library)\n",
        "- Implement hybrid search (BM25 + semantic)\n",
        "- Add citation verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**By Yotam Nachtomy-Katz** | ID: 211718366 | Haifa University | Information Retrieval Course | 01.02.26"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
