{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jerusalem RAG Explorer - Ingestion Pipeline\n",
        "\n",
        "**By Yotam Nachtomy-Katz** | ID: 211718366 | Haifa University\n",
        "\n",
        "This notebook demonstrates the ingestion pipeline: chunking documents, translating non-English text, generating embeddings, and building the FAISS index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "from dotenv import load_dotenv\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "# Check for API key\n",
        "if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "    print(\"WARNING: GEMINI_API_KEY not found. Translation will be skipped.\")\n",
        "else:\n",
        "    print(\"GEMINI_API_KEY loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "DATA_DIR = Path(\"../data/raw\")\n",
        "INDEX_DIR = Path(\"../data/index_v2\")\n",
        "TRANSLATION_CACHE_DIR = Path(\"../data/translations\")\n",
        "\n",
        "# Chunking parameters\n",
        "CHUNK_SIZE = 2000  # characters per chunk\n",
        "OVERLAP = 300      # overlap between chunks\n",
        "\n",
        "# Embedding model\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Language mappings\n",
        "LANGUAGE_NAMES = {\n",
        "    \"en\": \"English\",\n",
        "    \"la\": \"Latin\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"fr\": \"French\",\n",
        "    \"hy\": \"Armenian\",\n",
        "}\n",
        "\n",
        "def get_language_name(code: str) -> str:\n",
        "    return LANGUAGE_NAMES.get(code, code.upper())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Chunking\n",
        "\n",
        "Documents are split into overlapping segments to preserve context across chunk boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> list[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        part = text[i : i + chunk_size].strip()\n",
        "        if part:\n",
        "            chunks.append(part)\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Demonstrate chunking\n",
        "sample_text = \"A\" * 5000\n",
        "sample_chunks = chunk_text(sample_text)\n",
        "print(f\"Sample: {len(sample_text)} chars -> {len(sample_chunks)} chunks\")\n",
        "print(f\"Chunk sizes: {[len(c) for c in sample_chunks]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Metadata Extraction\n",
        "\n",
        "We extract metadata from the document headers that were added during fetching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_metadata_from_header(text: str) -> dict:\n",
        "    \"\"\"Extract metadata from document header.\"\"\"\n",
        "    metadata = {}\n",
        "    lines = text[:2000].split(\"\\n\")\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"---\"):\n",
        "            break\n",
        "        if \":\" in line:\n",
        "            key, _, value = line.partition(\":\")\n",
        "            key = key.strip().lower()\n",
        "            value = value.strip()\n",
        "            if key == \"title\":\n",
        "                metadata[\"title\"] = value\n",
        "            elif key == \"author\":\n",
        "                metadata[\"author\"] = value\n",
        "            elif key == \"language\":\n",
        "                metadata[\"language\"] = value\n",
        "            elif key == \"url\":\n",
        "                metadata[\"source_url\"] = value\n",
        "            elif key == \"source\":\n",
        "                metadata[\"source_repository\"] = value.lower()\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def detect_repository(filepath: Path) -> str:\n",
        "    \"\"\"Detect source repository from file path.\"\"\"\n",
        "    path_str = str(filepath).lower()\n",
        "    if \"gallica\" in path_str:\n",
        "        return \"gallica\"\n",
        "    if \"archive\" in path_str:\n",
        "        return \"archive\"\n",
        "    if \"wiki\" in path_str:\n",
        "        return \"wiki\"\n",
        "    return \"unknown\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Language Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_english(text: str) -> bool:\n",
        "    \"\"\"Check if text is primarily English.\"\"\"\n",
        "    try:\n",
        "        sample_start = min(500, len(text) // 4)\n",
        "        sample = text[sample_start : sample_start + 2000]\n",
        "        return detect(sample) == \"en\"\n",
        "    except LangDetectException:\n",
        "        return True\n",
        "\n",
        "\n",
        "def detect_language(text: str) -> str:\n",
        "    \"\"\"Detect language of text. Returns ISO 639-1 code.\"\"\"\n",
        "    # Check for explicit language markers in header\n",
        "    text_lower = text[:500].lower()\n",
        "    if \"language: la\" in text_lower:\n",
        "        return \"la\"\n",
        "    if \"language: ar\" in text_lower:\n",
        "        return \"ar\"\n",
        "    if \"language: el\" in text_lower or \"language: greek\" in text_lower:\n",
        "        return \"el\"\n",
        "    if \"language: fr\" in text_lower:\n",
        "        return \"fr\"\n",
        "\n",
        "    # Use langdetect for automatic detection\n",
        "    try:\n",
        "        sample_start = min(500, len(text) // 4)\n",
        "        sample = text[sample_start : sample_start + 2000]\n",
        "        detected = detect(sample)\n",
        "        # Map common misdetections\n",
        "        if detected in (\"hr\", \"sr\"):  # Often misdetects Latin\n",
        "            if any(w in text.lower() for w in [\"rex\", \"deus\", \"anno\", \"ecclesia\"]):\n",
        "                return \"la\"\n",
        "        return detected\n",
        "    except LangDetectException:\n",
        "        return \"en\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Translation Pipeline\n",
        "\n",
        "Non-English chunks are translated using the Gemini API with caching to avoid redundant API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TranslationCache:\n",
        "    \"\"\"Simple file-based cache for translations.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: Path):\n",
        "        self.cache_dir = cache_dir\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.index_file = self.cache_dir / \"index.json\"\n",
        "        self.index = self._load_index()\n",
        "\n",
        "    def _load_index(self) -> dict:\n",
        "        if self.index_file.exists():\n",
        "            return json.loads(self.index_file.read_text(encoding=\"utf-8\"))\n",
        "        return {}\n",
        "\n",
        "    def _save_index(self):\n",
        "        self.index_file.write_text(\n",
        "            json.dumps(self.index, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "        )\n",
        "\n",
        "    def _hash_text(self, text: str) -> str:\n",
        "        return hashlib.sha256(text.encode()).hexdigest()[:16]\n",
        "\n",
        "    def get(self, text: str, source_lang: str) -> Optional[str]:\n",
        "        key = f\"{source_lang}_{self._hash_text(text)}\"\n",
        "        if key in self.index:\n",
        "            cache_file = self.cache_dir / f\"{key}.txt\"\n",
        "            if cache_file.exists():\n",
        "                return cache_file.read_text(encoding=\"utf-8\")\n",
        "        return None\n",
        "\n",
        "    def put(self, text: str, source_lang: str, translation: str):\n",
        "        key = f\"{source_lang}_{self._hash_text(text)}\"\n",
        "        cache_file = self.cache_dir / f\"{key}.txt\"\n",
        "        cache_file.write_text(translation, encoding=\"utf-8\")\n",
        "        self.index[key] = {\n",
        "            \"source_lang\": source_lang,\n",
        "            \"original_len\": len(text),\n",
        "            \"translated_len\": len(translation),\n",
        "        }\n",
        "        self._save_index()\n",
        "\n",
        "\n",
        "# Initialize cache\n",
        "translation_cache = TranslationCache(TRANSLATION_CACHE_DIR)\n",
        "print(f\"Translation cache has {len(translation_cache.index)} entries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translation function using Gemini API\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "def translate_text(text: str, source_lang: str, cache: TranslationCache) -> str:\n",
        "    \"\"\"Translate text using Gemini API with caching.\"\"\"\n",
        "    # Check cache first\n",
        "    cached = cache.get(text, source_lang)\n",
        "    if cached:\n",
        "        return cached\n",
        "    \n",
        "    # Rate limit\n",
        "    time.sleep(15)  # 4 requests per minute\n",
        "    \n",
        "    # Build translation prompt\n",
        "    lang_name = get_language_name(source_lang)\n",
        "    prompt = f\"\"\"You are a scholarly translator specializing in medieval texts.\n",
        "Translate the following {lang_name} text to English.\n",
        "\n",
        "IMPORTANT:\n",
        "- Preserve proper nouns (names of people, places) in their common English forms\n",
        "- Keep historical terms with brief clarification if needed\n",
        "- Maintain the tone and style of medieval chronicles\n",
        "- If text contains OCR errors, do your best to interpret the intended meaning\n",
        "\n",
        "TEXT TO TRANSLATE:\n",
        "{text}\n",
        "\n",
        "ENGLISH TRANSLATION:\"\"\"\n",
        "\n",
        "    try:\n",
        "        client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        config = types.GenerateContentConfig(\n",
        "            temperature=0.1,\n",
        "            max_output_tokens=4096,\n",
        "        )\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-3-flash-preview\",\n",
        "            contents=prompt,\n",
        "            config=config,\n",
        "        )\n",
        "        translation = response.text or \"\"\n",
        "        \n",
        "        # Cache the result\n",
        "        if translation:\n",
        "            cache.put(text, source_lang, translation)\n",
        "        \n",
        "        return translation\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load and Process Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_file(filepath: Path) -> list[dict]:\n",
        "    \"\"\"Load a single file and create chunks with metadata.\"\"\"\n",
        "    text = filepath.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    # Extract metadata from header\n",
        "    metadata = extract_metadata_from_header(text)\n",
        "    repository = detect_repository(filepath)\n",
        "\n",
        "    # Detect or use declared language\n",
        "    declared_lang = metadata.get(\"language\", \"\").lower()\n",
        "    if declared_lang in (\"la\", \"latin\"):\n",
        "        lang = \"la\"\n",
        "    elif declared_lang in (\"ar\", \"arabic\"):\n",
        "        lang = \"ar\"\n",
        "    elif declared_lang in (\"el\", \"greek\"):\n",
        "        lang = \"el\"\n",
        "    elif declared_lang in (\"fr\", \"french\"):\n",
        "        lang = \"fr\"\n",
        "    elif is_english(text):\n",
        "        lang = \"en\"\n",
        "    else:\n",
        "        lang = detect_language(text)\n",
        "\n",
        "    # Create chunks\n",
        "    prefix = re.sub(r\"[^\\w\\-]\", \"_\", filepath.stem)\n",
        "    raw_chunks = chunk_text(text)\n",
        "\n",
        "    chunks = []\n",
        "    for i, chunk_text_content in enumerate(raw_chunks):\n",
        "        chunk = {\n",
        "            \"chunk_id\": f\"{prefix}_chunk_{i:03d}\",\n",
        "            \"source\": str(filepath),\n",
        "            \"text\": chunk_text_content,\n",
        "            \"language\": lang,\n",
        "            \"language_name\": get_language_name(lang),\n",
        "            \"is_translation\": False,\n",
        "            \"original_language\": None,\n",
        "            \"original_text\": None,\n",
        "            \"author\": metadata.get(\"author\"),\n",
        "            \"title\": metadata.get(\"title\"),\n",
        "            \"source_url\": metadata.get(\"source_url\"),\n",
        "            \"source_repository\": repository,\n",
        "        }\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all text files\n",
        "files = list(DATA_DIR.rglob(\"*.txt\"))\n",
        "print(f\"Found {len(files)} text files\")\n",
        "\n",
        "all_chunks = []\n",
        "for filepath in tqdm(files, desc=\"Loading files\"):\n",
        "    file_chunks = load_file(filepath)\n",
        "    all_chunks.extend(file_chunks)\n",
        "\n",
        "print(f\"Created {len(all_chunks)} chunks\")\n",
        "\n",
        "# Count by language\n",
        "lang_counts = {}\n",
        "for c in all_chunks:\n",
        "    lang = c.get(\"language\", \"en\")\n",
        "    lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
        "    \n",
        "print(\"\\nChunks by language:\")\n",
        "for lang, count in sorted(lang_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {get_language_name(lang)}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Translate Non-English Chunks\n",
        "\n",
        "**Note:** This step may take a long time due to API rate limits. You can limit the number of translations or skip this step entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for translation\n",
        "TRANSLATE = True  # Set to False to skip translation\n",
        "MAX_TRANSLATE = 15  # Limit translations (set to None for unlimited)\n",
        "\n",
        "if TRANSLATE and os.getenv(\"GEMINI_API_KEY\"):\n",
        "    non_english = [c for c in all_chunks if c.get(\"language\", \"en\") != \"en\"]\n",
        "    print(f\"Non-English chunks to translate: {len(non_english)}\")\n",
        "    \n",
        "    if MAX_TRANSLATE and MAX_TRANSLATE < len(non_english):\n",
        "        print(f\"Limiting to {MAX_TRANSLATE} chunks\")\n",
        "        non_english = non_english[:MAX_TRANSLATE]\n",
        "    \n",
        "    translated_count = 0\n",
        "    for chunk in tqdm(non_english, desc=\"Translating\"):\n",
        "        lang = chunk.get(\"language\", \"en\")\n",
        "        original_text = chunk[\"text\"]\n",
        "        \n",
        "        translated = translate_text(original_text, lang, translation_cache)\n",
        "        \n",
        "        if translated:\n",
        "            chunk[\"original_text\"] = original_text\n",
        "            chunk[\"text\"] = translated\n",
        "            chunk[\"is_translation\"] = True\n",
        "            chunk[\"original_language\"] = lang\n",
        "            translated_count += 1\n",
        "    \n",
        "    print(f\"\\nTranslated {translated_count} chunks\")\n",
        "else:\n",
        "    print(\"Skipping translation (disabled or no API key)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "print(f\"Loading embedding model: {MODEL_NAME}\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "# Get text from each chunk\n",
        "texts = [c[\"text\"] for c in all_chunks]\n",
        "print(f\"Embedding {len(texts)} chunks...\")\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    normalize_embeddings=True,\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "embeddings = np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Build FAISS Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "print(f\"Building FAISS index with {dim} dimensions...\")\n",
        "\n",
        "index = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"FAISS index built with {index.ntotal} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Index and Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save FAISS index\n",
        "faiss.write_index(index, str(INDEX_DIR / \"faiss.index\"))\n",
        "print(f\"Saved FAISS index to {INDEX_DIR / 'faiss.index'}\")\n",
        "\n",
        "# Save chunks with full metadata\n",
        "with open(INDEX_DIR / \"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Saved {len(all_chunks)} chunks to {INDEX_DIR / 'chunks.json'}\")\n",
        "\n",
        "# Save config\n",
        "config = {\n",
        "    \"chunk_size\": CHUNK_SIZE,\n",
        "    \"overlap\": OVERLAP,\n",
        "    \"embedding_model\": MODEL_NAME,\n",
        "    \"embedding_dim\": int(dim),\n",
        "    \"total_chunks\": len(all_chunks),\n",
        "}\n",
        "with open(INDEX_DIR / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(f\"Saved config to {INDEX_DIR / 'config.json'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test the Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query\n",
        "test_query = \"Battle of Hattin\"\n",
        "q_embedding = model.encode([test_query], normalize_embeddings=True)\n",
        "q_embedding = np.array(q_embedding, dtype=\"float32\")\n",
        "\n",
        "# Search\n",
        "scores, indices = index.search(q_embedding, 3)\n",
        "\n",
        "print(f\"Test query: '{test_query}'\\n\")\n",
        "for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "    chunk = all_chunks[idx]\n",
        "    print(f\"Result {i+1} (score: {score:.3f})\")\n",
        "    print(f\"  Chunk ID: {chunk['chunk_id']}\")\n",
        "    print(f\"  Language: {chunk['language_name']}\")\n",
        "    print(f\"  Preview: {chunk['text'][:200]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "The index is now ready! Proceed to:\n",
        "- **03_retrieval.ipynb** - Query the index and generate answers with Gemini"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
