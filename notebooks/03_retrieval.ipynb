{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jerusalem RAG Explorer - Retrieval and Question Answering\n",
        "\n",
        "**By Yotam Nachtomy-Katz** | ID: 211718366 | Haifa University\n",
        "\n",
        "This notebook demonstrates how to query the RAG system: semantic retrieval, context assembly, and answer generation with citations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(\"../.env\")\n",
        "\n",
        "# Check for API key\n",
        "if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "    print(\"WARNING: GEMINI_API_KEY not found. Answer generation will fail.\")\n",
        "else:\n",
        "    print(\"GEMINI_API_KEY loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Index and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "INDEX_DIR = Path(\"../data/index_v2\")\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Language mappings\n",
        "LANGUAGE_NAMES = {\n",
        "    \"en\": \"English\",\n",
        "    \"la\": \"Latin\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"fr\": \"French\",\n",
        "    \"hy\": \"Armenian\",\n",
        "}\n",
        "\n",
        "LANGUAGE_FLAGS = {\n",
        "    \"en\": \"ðŸ‡¬ðŸ‡§\",\n",
        "    \"la\": \"ðŸ‡»ðŸ‡¦\",\n",
        "    \"ar\": \"ðŸ‡¸ðŸ‡¦\",\n",
        "    \"el\": \"ðŸ‡¬ðŸ‡·\",\n",
        "    \"fr\": \"ðŸ‡«ðŸ‡·\",\n",
        "    \"hy\": \"ðŸ‡¦ðŸ‡²\",\n",
        "}\n",
        "\n",
        "def get_language_name(code: str) -> str:\n",
        "    return LANGUAGE_NAMES.get(code, code.upper())\n",
        "\n",
        "def get_language_flag(code: str) -> str:\n",
        "    return LANGUAGE_FLAGS.get(code, \"ðŸŒ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embedding model\n",
        "print(f\"Loading embedding model: {MODEL_NAME}\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "# Load FAISS index\n",
        "index_path = INDEX_DIR / \"faiss.index\"\n",
        "print(f\"Loading FAISS index from {index_path}\")\n",
        "index = faiss.read_index(str(index_path))\n",
        "print(f\"Index contains {index.ntotal} vectors\")\n",
        "\n",
        "# Load chunks metadata\n",
        "chunks_path = INDEX_DIR / \"chunks.json\"\n",
        "print(f\"Loading chunks from {chunks_path}\")\n",
        "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "print(f\"Loaded {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Retrieval Function\n",
        "\n",
        "The retrieval function embeds the query and searches for the most similar chunks in the FAISS index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve(\n",
        "    question: str,\n",
        "    top_k: int = 6,\n",
        "    languages: list[str] = None,\n",
        ") -> list[tuple[float, dict]]:\n",
        "    \"\"\"\n",
        "    Retrieve top-k relevant chunks for a question.\n",
        "    \n",
        "    Args:\n",
        "        question: Query string\n",
        "        top_k: Number of results to return\n",
        "        languages: Optional list of language codes to filter by\n",
        "        \n",
        "    Returns:\n",
        "        List of (score, chunk_dict) tuples\n",
        "    \"\"\"\n",
        "    # Encode query\n",
        "    q_emb = model.encode([question], normalize_embeddings=True)\n",
        "    q_emb = np.array(q_emb, dtype=\"float32\")\n",
        "\n",
        "    # Search - get more if filtering\n",
        "    search_k = top_k * 3 if languages else top_k\n",
        "    scores, ids = index.search(q_emb, search_k)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], ids[0]):\n",
        "        if idx < 0 or idx >= len(chunks):\n",
        "            continue\n",
        "\n",
        "        chunk = chunks[idx]\n",
        "\n",
        "        # Apply language filter\n",
        "        if languages:\n",
        "            chunk_lang = chunk.get(\"language\", \"en\")\n",
        "            orig_lang = chunk.get(\"original_language\")\n",
        "            if chunk_lang not in languages and orig_lang not in languages:\n",
        "                continue\n",
        "\n",
        "        results.append((float(score), chunk))\n",
        "\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query\n",
        "question = \"What happened at the Battle of Hattin?\"\n",
        "\n",
        "results = retrieve(question, top_k=5)\n",
        "\n",
        "print(f\"Query: '{question}'\\n\")\n",
        "print(f\"Found {len(results)} relevant chunks:\\n\")\n",
        "\n",
        "for i, (score, chunk) in enumerate(results):\n",
        "    lang = chunk.get(\"language\", \"en\")\n",
        "    flag = get_language_flag(lang)\n",
        "    lang_name = get_language_name(lang)\n",
        "    is_trans = chunk.get(\"is_translation\", False)\n",
        "    \n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Result {i+1}: {chunk['chunk_id']}\")\n",
        "    print(f\"Score: {score:.3f} | {flag} {lang_name}\" + (\" (translated)\" if is_trans else \"\"))\n",
        "    if chunk.get(\"author\"):\n",
        "        print(f\"Author: {chunk['author']}\")\n",
        "    if chunk.get(\"title\"):\n",
        "        print(f\"Title: {chunk['title']}\")\n",
        "    print(f\"\\nPreview:\\n{chunk['text'][:400]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Context Formatting\n",
        "\n",
        "Retrieved chunks are formatted with metadata for the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_context(\n",
        "    results: list[tuple[float, dict]],\n",
        "    include_original: bool = False,\n",
        "    max_chunk_len: int = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Format retrieved chunks into context for the LLM.\n",
        "    \n",
        "    Args:\n",
        "        results: List of (score, chunk) tuples\n",
        "        include_original: Include original text for translations\n",
        "        max_chunk_len: Optional max length per chunk\n",
        "        \n",
        "    Returns:\n",
        "        Formatted context string\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "\n",
        "    for score, chunk in results:\n",
        "        chunk_id = chunk.get(\"chunk_id\", \"unknown\")\n",
        "        lang = chunk.get(\"language\", \"en\")\n",
        "        lang_name = get_language_name(lang)\n",
        "        is_trans = chunk.get(\"is_translation\", False)\n",
        "        orig_lang = chunk.get(\"original_language\")\n",
        "\n",
        "        # Build header\n",
        "        header = f\"[{chunk_id}] (score: {score:.3f})\"\n",
        "        if orig_lang and orig_lang != \"en\":\n",
        "            orig_name = get_language_name(orig_lang)\n",
        "            header += f\" [Original: {orig_name}, Translated to English]\"\n",
        "        elif lang != \"en\":\n",
        "            header += f\" [{lang_name}]\"\n",
        "\n",
        "        # Get text\n",
        "        text = chunk.get(\"text\", \"\")\n",
        "        if max_chunk_len and len(text) > max_chunk_len:\n",
        "            text = text[:max_chunk_len] + \"...\"\n",
        "\n",
        "        part = f\"{header}\\n{text}\"\n",
        "\n",
        "        # Include original if requested\n",
        "        if include_original and is_trans:\n",
        "            original = chunk.get(\"original_text\", \"\")\n",
        "            if original:\n",
        "                if max_chunk_len and len(original) > max_chunk_len:\n",
        "                    original = original[:max_chunk_len] + \"...\"\n",
        "                part += f\"\\n\\n[Original {get_language_name(orig_lang)} text:]\\n{original}\"\n",
        "\n",
        "        parts.append(part)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format context from our results\n",
        "context = format_context(results, max_chunk_len=500)\n",
        "print(\"Formatted Context:\")\n",
        "print(\"=\" * 60)\n",
        "print(context[:2000])\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Prompt Templates\n",
        "\n",
        "Different response modes use different prompt structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a scholarly historian assistant specializing in the Crusades and the medieval Near East (1095-1291 CE).\n",
        "\n",
        "You work with PRIMARY SOURCES in multiple languages:\n",
        "- Latin chronicles (William of Tyre, Fulcher of Chartres, Gesta Francorum)\n",
        "- Arabic histories (Ibn al-Athir, Usama ibn Munqidh)\n",
        "- Byzantine Greek sources (Anna Comnena's Alexiad)\n",
        "- Modern scholarly analysis\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Answer ONLY using the provided CONTEXT - do not use outside knowledge\n",
        "2. EVERY factual claim must have a citation in format [ChunkID]\n",
        "3. When citing translated sources, note the original language\n",
        "4. For disputed events, present multiple perspectives if available\n",
        "5. Use proper medieval terminology (Outremer, Franks, Saracens where appropriate)\n",
        "6. Acknowledge when sources conflict or are insufficient\n",
        "\n",
        "If the answer is not in the CONTEXT, respond:\n",
        "\"The provided sources do not contain sufficient information to answer this question.\"\n",
        "\"\"\"\n",
        "\n",
        "MODES = {\n",
        "    \"default\": \"\"\"Provide a scholarly answer with proper citations.\n",
        "Format: Clear prose with [ChunkID] citations after each claim.\n",
        "Note the original language of sources when relevant.\"\"\",\n",
        "\n",
        "    \"chronology\": \"\"\"Create a detailed timeline of events.\n",
        "Format:\n",
        "- [YEAR or c. YEAR] Event description [ChunkID] (Source: Author/Work)\n",
        "\n",
        "Use approximate dates (c. 1187) when exact dates are unknown.\n",
        "Cross-reference multiple sources when available.\"\"\",\n",
        "\n",
        "    \"dossier\": \"\"\"Write a structured scholarly dossier with these sections:\n",
        "\n",
        "## Overview\n",
        "Brief introduction with key dates and significance\n",
        "\n",
        "## Primary Sources\n",
        "What do contemporary chronicles say? Cite with [ChunkID] and note languages.\n",
        "\n",
        "## Key Events\n",
        "Chronological narrative of major events\n",
        "\n",
        "## Historical Significance\n",
        "Why this matters in Crusade history\n",
        "\n",
        "## Source Notes\n",
        "List sources used, noting original languages (Latin, Arabic, Greek, etc.)\"\"\",\n",
        "\n",
        "    \"comparative\": \"\"\"Compare perspectives across different source traditions.\n",
        "\n",
        "## Western/Latin Sources\n",
        "Evidence from Frankish chronicles [ChunkID]\n",
        "\n",
        "## Eastern/Arabic Sources\n",
        "Evidence from Muslim historians [ChunkID]\n",
        "\n",
        "## Byzantine/Greek Sources\n",
        "Evidence from Eastern Christian sources [ChunkID]\n",
        "\n",
        "## Analysis\n",
        "- Points of agreement across traditions\n",
        "- Significant differences and possible reasons\n",
        "- Which details are unique to each tradition\"\"\",\n",
        "\n",
        "    \"claim_check\": \"\"\"Evaluate the historical claim using available sources.\n",
        "\n",
        "## Claim\n",
        "Restate the claim being evaluated\n",
        "\n",
        "## Primary Evidence\n",
        "What do medieval sources say? [ChunkID] with source language noted\n",
        "\n",
        "## Source Agreement\n",
        "Do sources agree, conflict, or provide different perspectives?\n",
        "\n",
        "## Verdict\n",
        "SUPPORTED / PARTIALLY SUPPORTED / NOT SUPPORTED / INSUFFICIENT EVIDENCE\n",
        "\n",
        "## Confidence\n",
        "HIGH / MEDIUM / LOW (based on source quality and agreement)\"\"\",\n",
        "}\n",
        "\n",
        "\n",
        "def build_prompt(mode: str, question: str, context: str) -> str:\n",
        "    \"\"\"Build a complete prompt for the LLM.\"\"\"\n",
        "    mode_instruction = MODES.get(mode, MODES[\"default\"])\n",
        "    \n",
        "    return f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "MODE: {mode.upper()}\n",
        "INSTRUCTION: {mode_instruction}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Answer Generation with Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "def ask_gemini(prompt: str) -> str:\n",
        "    \"\"\"Generate answer using Gemini API.\"\"\"\n",
        "    client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature=0.3,\n",
        "        max_output_tokens=4096,\n",
        "        top_p=0.95,\n",
        "        top_k=64\n",
        "    )\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=\"gemini-3-flash-preview\",\n",
        "        contents=prompt,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    return resp.text or \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Complete RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_question(\n",
        "    question: str,\n",
        "    mode: str = \"default\",\n",
        "    top_k: int = 6,\n",
        "    languages: list[str] = None,\n",
        ") -> tuple[str, list[tuple[float, dict]]]:\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieve + generate.\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        mode: Response mode (default, chronology, dossier, comparative, claim_check)\n",
        "        top_k: Number of sources to retrieve\n",
        "        languages: Optional language filter\n",
        "        \n",
        "    Returns:\n",
        "        (answer, results) tuple\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve relevant chunks\n",
        "    results = retrieve(question, top_k=top_k, languages=languages)\n",
        "    \n",
        "    if not results:\n",
        "        return \"No relevant sources found for this question.\", []\n",
        "    \n",
        "    # Step 2: Format context\n",
        "    context = format_context(results)\n",
        "    \n",
        "    # Step 3: Build prompt\n",
        "    prompt = build_prompt(mode, question, context)\n",
        "    \n",
        "    # Step 4: Generate answer\n",
        "    answer = ask_gemini(prompt)\n",
        "    \n",
        "    return answer, results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Interactive Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Default mode\n",
        "question = \"What happened at the Battle of Hattin?\"\n",
        "mode = \"default\"\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "answer, results = ask_question(question, mode=mode, top_k=6)\n",
        "\n",
        "print(\"\\nANSWER:\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SOURCES USED:\")\n",
        "for score, chunk in results:\n",
        "    lang = chunk.get(\"language\", \"en\")\n",
        "    flag = get_language_flag(lang)\n",
        "    print(f\"  {flag} {chunk['chunk_id']} (score: {score:.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Comparative mode\n",
        "question = \"How did different sources describe Saladin?\"\n",
        "mode = \"comparative\"\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "answer, results = ask_question(question, mode=mode, top_k=8)\n",
        "\n",
        "print(\"\\nANSWER:\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Chronology mode\n",
        "question = \"What were the major events of the First Crusade?\"\n",
        "mode = \"chronology\"\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "answer, results = ask_question(question, mode=mode, top_k=8)\n",
        "\n",
        "print(\"\\nANSWER:\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Try Your Own Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try your own question!\n",
        "your_question = \"Who was Baldwin IV of Jerusalem?\"  # <-- Change this\n",
        "your_mode = \"dossier\"  # Options: default, chronology, dossier, comparative, claim_check\n",
        "\n",
        "print(f\"Question: {your_question}\")\n",
        "print(f\"Mode: {your_mode}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "answer, results = ask_question(your_question, mode=your_mode, top_k=6)\n",
        "\n",
        "print(\"\\nANSWER:\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SOURCES:\")\n",
        "for score, chunk in results:\n",
        "    lang = chunk.get(\"language\", \"en\")\n",
        "    flag = get_language_flag(lang)\n",
        "    print(f\"  {flag} {chunk['chunk_id']} (score: {score:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete RAG pipeline:\n",
        "\n",
        "1. **Retrieval**: Embed the query and search the FAISS index for similar chunks\n",
        "2. **Context Assembly**: Format chunks with metadata for the LLM\n",
        "3. **Prompt Engineering**: Build structured prompts for different response modes\n",
        "4. **Generation**: Use Gemini to generate answers with citations\n",
        "\n",
        "The system supports multiple response modes:\n",
        "- **Default**: Scholarly prose with citations\n",
        "- **Chronology**: Timeline format\n",
        "- **Dossier**: Structured research report\n",
        "- **Comparative**: Cross-cultural analysis\n",
        "- **Claim Check**: Fact verification"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
